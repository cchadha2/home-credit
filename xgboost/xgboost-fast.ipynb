{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(df, num_folds, stratified = False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    \n",
    "    del df\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1000)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1000)\n",
    "    # Create arrays and dataframes to store results\n",
    "    xgb_val_preds = np.zeros(train_df.shape[0])\n",
    "    xgb_preds = np.zeros(test_df.shape[0])\n",
    "    \n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    print(\"Starting XGBoost. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        \n",
    "        data_train, data_valid = train_df[feats].iloc[train_idx], train_df[feats].iloc[valid_idx]\n",
    "        label_train, label_valid = train_df['TARGET'].iloc[train_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "                \n",
    "        dtrain_xgb = xgb.DMatrix(data_train, label_train)\n",
    "        dvalid_xgb = xgb.DMatrix(data_valid, label_valid)\n",
    "        dtest_xgb = xgb.DMatrix(test_df[feats])\n",
    "        \n",
    "        params_xgb = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'boosting_type': 'gbtree',\n",
    "            'nthread': 7,\n",
    "            'learning_rate': 0.02,  # 02,\n",
    "            'colsample_bytree': 0.9497036,\n",
    "            'subsample': 0.8715623,\n",
    "            'max_depth': 8,\n",
    "            'reg_alpha': 0.041545473,\n",
    "            'reg_lambda': 0.0735294,\n",
    "            'min_split_gain': 0.0222415,\n",
    "            'min_child_weight': 60, # 39.3259775,\n",
    "            'seed': 0,\n",
    "            'eval_metric': 'auc',\n",
    "            'verbose': 100\n",
    "        }\n",
    "        \n",
    "        xgb_clf = xgb.train(\n",
    "            params=params_xgb,\n",
    "            dtrain=dtrain_xgb,\n",
    "            num_boost_round=500,\n",
    "            evals=[(dtrain_xgb, 'train'), (dvalid_xgb, 'valid')],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=100\n",
    "        )\n",
    "        \n",
    "        xgb_val_preds[valid_idx] = xgb_clf.predict(dvalid_xgb)\n",
    "        xgb_preds += xgb_clf.predict(dtest_xgb) / folds.n_splits\n",
    "        \n",
    "        print('XGB fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(dvalid_xgb.get_label(), xgb_val_preds[valid_idx])))\n",
    "        del xgb_clf, dtrain_xgb, dvalid_xgb, data_train, data_valid, label_train, label_valid\n",
    "\n",
    "\n",
    "    print('XGB Full AUC score %.6f' % roc_auc_score(lgb_train['TARGET'], xgb_val_preds))\n",
    "\n",
    "    # Write submission file\n",
    "    pred_df = test_df[['SK_ID_CURR']].copy()\n",
    "    pred_df['TARGET'] = xgb_preds\n",
    "    pred_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks You Guillaume Martin for the Awesome Memory Optimizer!\n",
    "# https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        #else: df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1812.91 MB\n",
      "Memory usage after optimization is: 698.53 MB\n",
      "Decreased by 61.5%\n",
      "Starting XGBoost. Train shape: (307511, 667), test shape: (48744, 667)\n",
      "[0]\ttrain-auc:0.754114\tvalid-auc:0.718756\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 100 rounds.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    df = pd.read_csv('../data/processed_data_2.2.csv')\n",
    "    df = reduce_mem_usage(df)\n",
    "    with timer(\"Ran model blend with kfold\"):\n",
    "        xgboost(df, num_folds= 5, stratified = True) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submission_file_name = \"../predictions/xgb_pred.csv\"\n",
    "    with timer(\"Full model run\"):\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
